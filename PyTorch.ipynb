{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc67451",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "PyTorch is very similar to Keras. Apparently, Keras was the first foray of deep learning models into the python environment. PyTorch also uses a TensorFlow back end, but the API is customised and somehow better than Keras. I could not tell you why, but the documentation from oracle and the like state this, and I am not one to argue with those that know better. \n",
    "\n",
    "I can also tell you that PyTorch computes its inputs forwards, then plays them backwards to compute gradients. This is likely to cut down on computing power required to predict things. \n",
    "\n",
    "As a very junior data person with very little exposure to programming and being new at python, I have chosen to adapt a tutorial to this situation. Link is in the Bibliography. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b7270eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1abb9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671d9b6",
   "metadata": {},
   "source": [
    "Lets start with the reading of the CSV and dividing the data by stock number. Now this method essentially adds error as I intend to train this model, and use it to predict other stocks. This really would increase error, however in terms of predicting stocks this is likely to be slightly more accurate than a random number generator providing a predictor of the stocks. In my mind, this is the benchmark for purchasing stocks in this manner, and any thing that provides a slightly more than 50% accuracy in real life is good to go.\n",
    "\n",
    "I must add, this is not how I would pick my stocks. I tend to purchase stocks in either:\n",
    "\n",
    "1. Index funds with low fees, or\n",
    "\n",
    "2. Companies that I know and use personally. They all have good management or governance, is below it's inherent value, and has some sort of moat. (Well if possible, I have been purchasing petrochemical stocks with my petrol savings from driving a prius so that I can thank my ute driving friends for giving me money as they drive down the road.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "19dd086d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjustmentFactor</th>\n",
       "      <th>ExpectedDividend</th>\n",
       "      <th>SupervisionFlag</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170104_1377</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1377</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>150800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>20170105_1377</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1377</td>\n",
       "      <td>3340.0</td>\n",
       "      <td>3355.0</td>\n",
       "      <td>3295.0</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>155700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.004525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>20170106_1377</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>1377</td>\n",
       "      <td>3320.0</td>\n",
       "      <td>3335.0</td>\n",
       "      <td>3260.0</td>\n",
       "      <td>3315.0</td>\n",
       "      <td>153300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.033033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5599</th>\n",
       "      <td>20170110_1377</td>\n",
       "      <td>2017-01-10</td>\n",
       "      <td>1377</td>\n",
       "      <td>3325.0</td>\n",
       "      <td>3360.0</td>\n",
       "      <td>3310.0</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>192500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.046584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>20170111_1377</td>\n",
       "      <td>2017-01-11</td>\n",
       "      <td>1377</td>\n",
       "      <td>3260.0</td>\n",
       "      <td>3295.0</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>3220.0</td>\n",
       "      <td>741200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.010386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              RowId        Date  SecuritiesCode    Open    High     Low  \\\n",
       "4     20170104_1377  2017-01-04            1377  3270.0  3350.0  3270.0   \n",
       "1869  20170105_1377  2017-01-05            1377  3340.0  3355.0  3295.0   \n",
       "3734  20170106_1377  2017-01-06            1377  3320.0  3335.0  3260.0   \n",
       "5599  20170110_1377  2017-01-10            1377  3325.0  3360.0  3310.0   \n",
       "7464  20170111_1377  2017-01-11            1377  3260.0  3295.0  3180.0   \n",
       "\n",
       "       Close  Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag  \\\n",
       "4     3330.0  150800               1.0               NaN            False   \n",
       "1869  3305.0  155700               1.0               NaN            False   \n",
       "3734  3315.0  153300               1.0               NaN            False   \n",
       "5599  3330.0  192500               1.0               NaN            False   \n",
       "7464  3220.0  741200               1.0               NaN            False   \n",
       "\n",
       "        Target  \n",
       "4     0.003026  \n",
       "1869  0.004525  \n",
       "3734 -0.033033  \n",
       "5599  0.046584  \n",
       "7464 -0.010386  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/crump/Documents/University/755 Data Analytics/jpx_tokyo_se_prediction/train_files/stock_prices.csv' )\n",
    "\n",
    "\n",
    "secs = [1377]\n",
    "\n",
    "\n",
    "df = df[df['SecuritiesCode'].isin(secs)]\n",
    "df.Close=df.Close.ffill()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39a5b6",
   "metadata": {},
   "source": [
    "This is normalising the training data. For this method, we can use -1 to 1 scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "041e0f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12797167],\n",
       "       [-0.15326252],\n",
       "       [-0.14314618],\n",
       "       ...,\n",
       "       [-0.32524026],\n",
       "       [-0.35053111],\n",
       "       [-0.28983308]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = df.Close\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "price = scaler.fit_transform(price.values.reshape(-1,1))\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ec1e7afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4          0.003026\n",
       "1869       0.004525\n",
       "3734      -0.033033\n",
       "5599       0.046584\n",
       "7464      -0.010386\n",
       "             ...   \n",
       "2322536    0.003200\n",
       "2324536   -0.007974\n",
       "2326536    0.019293\n",
       "2328536    0.009464\n",
       "2330536    0.026562\n",
       "Name: Target, Length: 1202, dtype: float64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.Target\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817f14a",
   "metadata": {},
   "source": [
    "Here we are splitting the data into training and testings sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8f495a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(price, lookback):\n",
    "    data_raw = numpy.array(price) # convert to numpy array\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - lookback): \n",
    "        data.append(data_raw[index: index + lookback])\n",
    "    \n",
    "    data = np.array(data);\n",
    "    test_set_size = int(np.round(0.2*data.shape[0]));\n",
    "    train_set_size = data.shape[0] - (test_set_size);\n",
    "    \n",
    "    x_train = data[:train_set_size,:-1,:]\n",
    "    y_train = data[:train_set_size,-1,:]\n",
    "    \n",
    "    bx_test = data[train_set_size:,:-1]\n",
    "    by_test = data[train_set_size:,-1,:]\n",
    "    \n",
    "    return [x_train, y_train, bx_test, by_test]\n",
    "lookback = 20 # choose sequence length\n",
    "x_train, y_train,x_test, y_test = split_data(price, lookback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28580002",
   "metadata": {},
   "source": [
    "Here we are applying tensors to the arrays we created above. Essentially, they are just another way of organising the data so in a format that the model is expecting. NumPy can also do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "62465698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e17ea4",
   "metadata": {},
   "source": [
    "This is setting the conditions. I also use this phrase in my work... Usually for explaining why I am doing something odd. Here we are telling the computer how many layers, how many times we are sending the model through the data and so on. You will note that there is not many epochs. This is to make computation easy for my little 8Gb computer. In the cloud, we would have a look at how driving up epochs drives up accuracy then overfitting and \"find the elbow\" where we look at how many epochs would create the most accurate model without driving up overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b8bb1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56199b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb0e96a8",
   "metadata": {},
   "source": [
    "Here we are creating the LSTM for the model to be trained on. Below that is how we are training the model using the LSTM we created immediately below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "65b0aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "39f49800",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef1ee44",
   "metadata": {},
   "source": [
    "Now we are setting the model to the data. Otherwise know as training. It goes through the data 5 times (because that is what we set it to) for which we can use the model to look at new data, compare and make a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3676d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  0.04907715320587158\n",
      "Epoch  1 MSE:  0.035570088773965836\n",
      "Epoch  2 MSE:  0.021377667784690857\n",
      "Epoch  3 MSE:  0.02958555892109871\n",
      "Epoch  4 MSE:  0.026135524734854698\n",
      "Epoch  5 MSE:  0.02078162506222725\n",
      "Epoch  6 MSE:  0.015142541378736496\n",
      "Epoch  7 MSE:  0.016536669805645943\n",
      "Epoch  8 MSE:  0.01943903975188732\n",
      "Epoch  9 MSE:  0.018666766583919525\n",
      "Epoch  10 MSE:  0.014879148453474045\n",
      "Epoch  11 MSE:  0.012219926342368126\n",
      "Epoch  12 MSE:  0.013683983124792576\n",
      "Epoch  13 MSE:  0.015316862612962723\n",
      "Epoch  14 MSE:  0.012629331089556217\n",
      "Epoch  15 MSE:  0.010566500946879387\n",
      "Epoch  16 MSE:  0.011148560792207718\n",
      "Epoch  17 MSE:  0.011514091864228249\n",
      "Epoch  18 MSE:  0.0105049517005682\n",
      "Epoch  19 MSE:  0.009473678655922413\n",
      "Epoch  20 MSE:  0.009326554834842682\n",
      "Epoch  21 MSE:  0.00951460562646389\n",
      "Epoch  22 MSE:  0.009148440323770046\n",
      "Epoch  23 MSE:  0.00836796686053276\n",
      "Epoch  24 MSE:  0.008179725147783756\n",
      "Epoch  25 MSE:  0.008552486076951027\n",
      "Epoch  26 MSE:  0.008244350552558899\n",
      "Epoch  27 MSE:  0.007422792259603739\n",
      "Epoch  28 MSE:  0.00709881167858839\n",
      "Epoch  29 MSE:  0.0072599551640450954\n",
      "Epoch  30 MSE:  0.007286363281309605\n",
      "Epoch  31 MSE:  0.0069321030750870705\n",
      "Epoch  32 MSE:  0.0064901262521743774\n",
      "Epoch  33 MSE:  0.006314673461019993\n",
      "Epoch  34 MSE:  0.006341319531202316\n",
      "Epoch  35 MSE:  0.006248760968446732\n",
      "Epoch  36 MSE:  0.006034791935235262\n",
      "Epoch  37 MSE:  0.005921307019889355\n",
      "Epoch  38 MSE:  0.005877532530575991\n",
      "Epoch  39 MSE:  0.005764327943325043\n",
      "Training time: 5.24390721321106\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model(x_train)\n",
    "\n",
    "    loss = criterion(y_train_pred, y_train_lstm)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7719a3",
   "metadata": {},
   "source": [
    "Here we are looking at the accuracy of the model that we have built. The root mean squared error is a measure of how much error there is in our predictions. As you can see there is a lot of error in our predictions as we have only trained the model 5 times. I have put the model through 5 epochs which makes it very inaccurate indeed. At 40 the RMSE is below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "866b8875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 75.05 RMSE\n",
      "Test Score: 58.21 RMSE\n"
     ]
    }
   ],
   "source": [
    "import math, time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# make predictions\n",
    "y_test_pred = model(x_test)\n",
    "\n",
    "# invert predictions\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
    "y_train = scaler.inverse_transform(y_train_lstm.detach().numpy())\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
    "y_test = scaler.inverse_transform(y_test_lstm.detach().numpy())\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "lstm.append(trainScore)\n",
    "lstm.append(testScore)\n",
    "lstm.append(training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e3934",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor?msclkid=efc3bd80ce6b11ec92e6d5b2ca0d6fc3\n",
    "\n",
    "https://www.educba.com/pytorch-tensors/?msclkid=7c5bd64bce7911ec97766487de736050\n",
    "\n",
    "https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor?msclkid=efc3bd80ce6b11ec92e6d5b2ca0d6fc3\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "https://stackoverflow.com/questions/30197943/numpy-ndarray-object-has-no-attribute-remove"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
